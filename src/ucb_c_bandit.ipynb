{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandit Based on Upper Confidence Bound for E-commerce Platform\n",
    "\n",
    "This is a contextual bandit algorithm designed for an e-commerce platform with four contexts or channels. There are five models that run in each context. Model selection is based on the Upper Confidence Bound (UCB) algorithm.\n",
    "\n",
    "Author: Okwudili Ezeme\n",
    "Date: 2021-10-15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ContextualBandit:\n",
    "    \"\"\"A contextual bandit algorithm designed for an e-commerce platform with four contexts or channels.\n",
    "    \n",
    "    Attributes:\n",
    "        n_channels (int): The number of channels or contexts.\n",
    "        n_models (int): The number of models to run in each context.\n",
    "        rewards (numpy.ndarray): A 2D array of rewards for each channel and model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_channels = 4\n",
    "        self.n_models = 5\n",
    "        self.rewards = np.array([[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                                 [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "                                 [0.2, 0.3, 0.5, 0.1, 0.4],\n",
    "                                 [0.4, 0.1, 0.2, 0.3, 0.5]])\n",
    "    \n",
    "    def get_reward(self, channel, model):\n",
    "        \"\"\"Get the reward for a given channel and model.\n",
    "        \n",
    "        Args:\n",
    "            channel (int): The channel or context.\n",
    "            model (int): The model to select.\n",
    "            \n",
    "        Returns:\n",
    "            int: The reward for the selected model.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        if np.random.rand() < self.rewards[channel][model]:\n",
    "            reward = 1\n",
    "        return reward\n",
    "    \n",
    "class UCB:\n",
    "    \"\"\"An implementation of the Upper Confidence Bound (UCB) algorithm.\n",
    "    \n",
    "    Attributes:\n",
    "        n_models (int): The number of models to select from.\n",
    "        model_counts (numpy.ndarray): An array of counts for each model.\n",
    "        model_rewards (numpy.ndarray): An array of rewards for each model.\n",
    "        total_counts (int): The total number of counts across all models.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_models):\n",
    "        self.n_models = n_models\n",
    "        self.model_counts = np.zeros(n_models)\n",
    "        self.model_rewards = np.zeros(n_models)\n",
    "        self.total_counts = 0\n",
    "        self.channel_model_rewards = np.zeros((4, 5))\n",
    "        \n",
    "    def select_model(self, channel_rewards):\n",
    "        \"\"\"Select the best model based on the UCB algorithm.\n",
    "        \n",
    "        Args:\n",
    "            channel_rewards (numpy.ndarray): An array of rewards for each model in the current channel.\n",
    "            \n",
    "        Returns:\n",
    "            int: The index of the selected model.\n",
    "        \"\"\"\n",
    "        ucb_values = np.zeros(self.n_models)\n",
    "        for i in range(self.n_models):\n",
    "            if self.model_counts[i] == 0:\n",
    "                ucb_values[i] = np.inf\n",
    "            else:\n",
    "                average_reward = self.channel_model_rewards[:, i].sum() / self.model_counts[i]\n",
    "                exploration = np.sqrt(2 * np.log(self.total_counts) / self.model_counts[i])\n",
    "                ucb_values[i] = average_reward + exploration\n",
    "        model = np.argmax(ucb_values)\n",
    "        self.model_counts[model] += 1\n",
    "        self.total_counts += 1\n",
    "        reward = channel_rewards[model]\n",
    "        self.model_rewards[model] += reward\n",
    "        self.channel_model_rewards[:, model] += channel_rewards\n",
    "        return model\n",
    "    \n",
    "def train_bandit(bandit, ucb, epochs):\n",
    "    \"\"\"Train the contextual bandit using the UCB algorithm.\n",
    "    \n",
    "    Args:\n",
    "        bandit (ContextualBandit): The contextual bandit to train.\n",
    "        ucb (UCB): The UCB algorithm to use for model selection.\n",
    "        epochs (int): The number of training epochs.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An array of rewards for each epoch.\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(epochs)\n",
    "    for i in range(epochs):\n",
    "        channel = np.random.randint(bandit.n_channels)\n",
    "        model = ucb.select_model(bandit.rewards[channel])\n",
    "        reward = bandit.get_reward(channel, model)\n",
    "        ucb.model_rewards[model] += reward\n",
    "        ucb.model_counts[model] += 1\n",
    "        ucb.channel_model_rewards[channel, model] += reward\n",
    "        rewards[i] = reward\n",
    "    return rewards\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bandit = ContextualBandit()\n",
    "    ucb = UCB(bandit.n_models)\n",
    "    rewards = train_bandit(bandit, ucb, 100000)\n",
    "    print('Average reward:', np.mean(rewards))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
