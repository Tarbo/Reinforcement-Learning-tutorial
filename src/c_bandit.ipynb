{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Use Case\n",
    "In this noteboook, I will implement a contextual bandit algorithm for online shopping platform. The *contexts* are derived using a B2C business model but can easily be leveraged for other types of business \n",
    "model. In a bandit model for an online ecommerce platform, the contexts could be various characteristics of the user or the item. Here are a few examples:\n",
    "\n",
    "1. User demographics: Age, gender, location, etc.\n",
    "2. User behavior: Past purchases, browsing history, click patterns, etc.\n",
    "3. Time: Time of day, day of the week, season, etc.\n",
    "4. Item characteristics: Category, price, brand, ratings, etc.\n",
    "5. Current context: What page the user is on, what they searched for, etc.\n",
    "\n",
    "These contexts can be used to personalize the recommendations made by the bandit algorithm. For example, the algorithm might recommend different products to a user who is browsing in the morning compared to the evening, or to a user who has a history of purchasing electronics compared to a user who typically buys books.\n",
    "\n",
    "There are several algorithms used in bandit models for online ecommerce platforms. Here are a few examples:\n",
    "\n",
    "1. **Epsilon-Greedy Algorithm**: This is a simple method where the algorithm explores with a probability of epsilon and exploits the best option otherwise. \n",
    "\n",
    "2. **Upper Confidence Bound (UCB) Algorithm**: This algorithm balances exploration and exploitation by choosing the option with the highest upper confidence bound.\n",
    "\n",
    "3. **Thompson Sampling**: This is a probabilistic algorithm that chooses an option based on the probability that it is the best option.\n",
    "\n",
    "4. **Contextual Bandit Algorithms**: These algorithms take into account the context (user demographics, time of day, etc.) when choosing an option.\n",
    "\n",
    "5. **Gradient Bandit Algorithms**: These algorithms use a gradient ascent method to update the preference for each action based on the received reward.\n",
    "\n",
    "Each of these algorithms has its own strengths and weaknesses, and the choice of algorithm depends on the specific requirements of the ecommerce platform.\n",
    "\n",
    "## 1.1 Algorithm Selection: \n",
    "I will be implementing a contextual bandit model using *Grandient Bandit Algorithms* to power the model selection given a context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class ContextualBandit:\n",
    "    \"\"\"A class that defines the contextual bandit environment.\n",
    "\n",
    "    Attributes:\n",
    "        state (int): The current state of the environment.\n",
    "        channels (list): A list of the channels in the environment.\n",
    "        models (list): A list of the models in the environment.\n",
    "        rewards (numpy.ndarray): A 2D array of the rewards for each model and channel.\n",
    "        num_channels (int): The number of channels (contexts) in the environment.\n",
    "        num_models (int): The number of models in the environment.\n",
    "        total_reward (float): The total reward obtained by the agent.\n",
    "        action_count (numpy.ndarray): A 2D array of the number of times each action has been taken for each state.\n",
    "        total_reward_per_model_per_channel (numpy.ndarray): A 2D array of the total reward obtained by each model for each channel.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the ContextualBandit class.\"\"\"\n",
    "        self.state = 0\n",
    "        # Define the four channels\n",
    "        self.channels = ['Channel 1', 'Channel 2', 'Channel 3', 'Channel 4']\n",
    "        # Define the nine models\n",
    "        self.models = ['Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5', 'Model 6', 'Model 7', 'Model 8', 'Model 9']\n",
    "        # Define the rewards for each model\n",
    "        self.rewards = np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "                                 [0.2, 0.3, 0.4, 0.1],\n",
    "                                 [0.3, 0.4, 0.1, 0.2],\n",
    "                                 [0.4, 0.1, 0.2, 0.3],\n",
    "                                 [0.5, 0.6, 0.7, 0.8],\n",
    "                                 [0.6, 0.7, 0.8, 0.5],\n",
    "                                 [0.7, 0.8, 0.5, 0.6],\n",
    "                                 [0.8, 0.5, 0.6, 0.7],\n",
    "                                 [0.9, 0.9, 0.9, 0.9]])\n",
    "        self.num_channels = len(self.channels)\n",
    "        self.num_models = len(self.models)\n",
    "        self.total_reward = 0\n",
    "        self.action_count = np.zeros((self.num_channels, self.num_models))\n",
    "        self.total_reward_per_model_per_channel = np.zeros((self.num_channels, self.num_models))\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Gets a random state from the environment.\n",
    "\n",
    "        Returns:\n",
    "            int: A random state from the environment.\n",
    "        \"\"\"\n",
    "        # Randomly select a channel. For deployment, this would be the channel that the user is currently on.\n",
    "        self.state = np.random.randint(0, self.num_channels)\n",
    "        return self.state\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        \"\"\"Gets the reward for a given action in the current state.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take in the current state.\n",
    "\n",
    "        Returns:\n",
    "            float: The reward for the given action in the current state.\n",
    "        \"\"\"\n",
    "        # Get the reward for the selected model and channel\n",
    "        reward = self.rewards[action, self.state]\n",
    "        # Generate a random noise to add to the reward\n",
    "        noise = np.random.randn(1)/10.0\n",
    "        # Add the noise to the reward\n",
    "        reward += noise\n",
    "        # Update the total reward\n",
    "        self.total_reward += reward\n",
    "        # Update the reward per model per channel\n",
    "        self.total_reward_per_model_per_channel[self.state, action] += reward\n",
    "        # Increment the action count for the current state and action\n",
    "        self.action_count[self.state, action] += 1\n",
    "        return reward\n",
    "\n",
    "    def get_action(self, model, step):\n",
    "        \"\"\"Gets an action for a given model using the Upper Confidence Bound algorithm.\n",
    "\n",
    "        Args:\n",
    "            model (int): The model to select an action for.\n",
    "            step (int): The current step of the algorithm.\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take for the given model.\n",
    "        \"\"\"\n",
    "        # Calculate the Upper Confidence Bound for each action\n",
    "        ucb = np.zeros(self.num_models)\n",
    "        for i in range(self.num_models):\n",
    "            if np.sum(self.action_count[self.state, :]) == 0:\n",
    "                ucb[i] = np.inf\n",
    "            else:\n",
    "                ucb[i] = self.total_reward_per_model_per_channel[self.state, i] / self.action_count[self.state, i] + math.sqrt(2 * math.log(step) / self.action_count[self.state, i])\n",
    "        # Select the action with the highest UCB\n",
    "        action = np.argmax(ucb)\n",
    "        return action\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"A class that defines the agent that interacts with the contextual bandit environment.\n",
    "\n",
    "    Attributes:\n",
    "        model (tensorflow.python.keras.engine.training.Model): The neural network model used by the agent.\n",
    "    \"\"\"\n",
    "            \"\"\"\n",
    "    def __init__(self, lr, state_size, action_size):\n",
    "        \"\"\"Initializes the Agent class.\n",
    "\n",
    "        Args:\n",
    "            lr (float): The learning rate for the neural network model.\n",
    "            state_size (int): The size of the state space.\n",
    "            action_size (int): The size of the action space.\n",
    "            \"\"\"\n",
    "        self.state_in = tf.keras.layers.Input(shape=(1,), dtype=tf.int32)\n",
    "        state_in_OH = tf.one_hot(indices=self.state_in, depth=state_size)\n",
    "        output = Dense(units=action_size, activation=tf.nn.sigmoid, kernel_initializer=tf.ones_initializer())(state_in_OH)\n",
    "        self.model = tf.keras.models.Model(inputs=self.state_in, outputs=output)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr))\n",
    "        \n",
    "    def get_action(self, state, bandit, step):\n",
    "        \"\"\"Gets an action for a given state using the Upper Confidence Bound algorithm.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): The state to select an action for.\n",
    "            bandit (ContextualBandit): The contextual bandit environment.\n",
    "            step (int): The current step of the algorithm.\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take for the given state.\n",
    "        \"\"\"\n",
    "        # Get the action for the current state using the Upper Confidence Bound algorithm\n",
    "        action = bandit.get_action(self.model.predict(state)[0], step)\n",
    "        return action\n",
    "\n",
    "        Returns:\n",
    "            int: An action for the given state.\n",
    "        \"\"\"\n",
    "        probs = self.model.predict(state)[0]\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def train(self, state, action, reward):\n",
    "        \"\"\"Trains the neural network model.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): The state to train the model on.\n",
    "            action (int): The action taken in the given state.\n",
    "            reward (float): The reward received for taking the given action in the given state.\n",
    "        \"\"\"\n",
    "        action_one_hot = tf.one_hot(indices=action, depth=action_size)\n",
    "        self.model.train_on_batch(state, action_one_hot, sample_weight=reward)\n",
    "\n",
    "\n",
    "def train_bandit(agent, bandit, num_episodes):\n",
    "    \"\"\"Trains the agent on the contextual bandit environment.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): The agent to train.\n",
    "        bandit (ContextualBandit): The contextual bandit environment to train the agent on.\n",
    "        num_episodes (int): The number of episodes to train the agent on.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D array of the total rewards for each channel and model.\n",
    "    \"\"\"\n",
    "    total_episodes = num_episodes  # Set total number of episodes to train agent on.\n",
    "    total_reward = np.zeros([bandit.num_channels, bandit.num_models])  # Set scoreboard for bandit (4x9).\n",
    "    e = 0.1  # Set the chance of taking a random action.\n",
    "\n",
    "    # Launch the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        i = 0\n",
    "        while i < total_episodes:\n",
    "            s = bandit.get_state()  # Get a state from the environment.\n",
    "            # Choose either a random action or one from our network.\n",
    "            if np.random.rand(1) < e:\n",
    "                a = bandit.get_action()\n",
    "            else:\n",
    "                a = agent.get_action(np.array([s]))\n",
    "            r = bandit.get_reward(a)  # Get our reward for taking an action given a bandit.\n",
    "            # Update the network.\n",
    "            agent.train(np.array([s]), a, r)\n",
    "            # Update our running tally of scores.\n",
    "            total_reward[s, a] += r\n",
    "            if i % 100 == 0:\n",
    "                print(\"Mean reward for each of the \" + str(bandit.num_channels) + \" channels: \" + str(np.mean(total_reward, axis=1)))\n",
    "                            i += 1\n",
    "    return total_reward\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
